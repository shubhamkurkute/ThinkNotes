## Advanced Prompt Engineering Techniques

#### What is Prompt?
Prompt is the instructions given to LLM to perform a particular task and to get the desired output from the LLM. Prompt is not the question but the mini specification.

### Prompt Elements:
- Prompt elements refer to the components that constitute a complete and effective prompt when interacting with AI models, especially LLMs.
- The main prompt elements typically include:
	- **Instruction:** The steps given about the task that LLM should consider while performing a particular task.
	- **Context:** Context is the background information about the task to be performed
	- **Input data:** The data you feed to the LLM, which helps the model to help on which to work on.
	- Output indicator: The format of the expected data from the LLM.

### Advanced Prompt Engineering
- It refers to the developement of more sophisticated techniques for interacting with language models beyond the basics.
- Advanced prompt is like making model how to think before it speaks.
- **LLM setting for Optimal prompting:**
	- **Tempreture** decides the randomness of the output generated by the LLM. It ranges from 0 to 1.
		- A **lower temperature** (like 0.2 or 0.3) makes responses more _focused, factual, and repetitive_ — ideal for coding, reports, or analytics.
		- A **higher temperature** (like 0.8 or 0.9) adds _creativity and variation_ — perfect for storytelling, brainstorming, or marketing copy.”
	- **Top-p (Nucleus Sampling)** – controls how _broad or narrow_ the model’s word choice can be. It ranges from 0 to 1
		- 1.0 = consider all possibel tokens
		- 0.0 = only the top 10% most likely tokens.
	- **Max Length** – limits how _long_ the response can go.
	- **Frequency Penalty:** It penalizes the tokens based on their frequency in the prompt and response, thereby enhaning the diversity of the language used
	- **Presence Penalty:** Implement this penalty to prevent repeated tokens, irrespective of theri frequency, promoting original and varied outpurs from the model.