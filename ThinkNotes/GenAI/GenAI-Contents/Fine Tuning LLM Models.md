### Quantization
Conversion from higher memory format to a lower memory format.
In neural network, weights and bias are stored in FP 32 bits. Where FP is full precision. Inference can be done quickly.
While doing quantization there is loss of data which ultimately leads to loss of accuracy.

#### Full Precision / Half precision
- Converting high memory format to lower memory format i.e last memory format is called is Full precision
- Converting the high memory format to half the memory format is called as half precision.

#### Caliberation
The way or mathematical formula to perform quantization.

#### How to perform Quantization
1. Symmetric Quantization:
	- Symmetric uint8 quantization:
		- 

